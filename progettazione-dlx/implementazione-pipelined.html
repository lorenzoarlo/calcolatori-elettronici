<!DOCTYPE html>
<html lang="IT">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="stylesheet" href="./../styles/style.css" />
    <link rel="stylesheet" href="./../styles/index-style.css" />
    <link rel="stylesheet" href="./../styles/main-index-page-style.css" />
    
        <link rel="stylesheet" href="./../styles/content-style.css" />
    
    <style>:root { --bg-clr: #FFD700; --fg-clr: #262626; }</style>
    <meta name="application-name" content="Calcolatori elettronici" />
    <meta name="apple-mobile-web-app-title" content="Calcolatori elettronici" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="#FFD700" />
    <link rel="apple-touch-icon" sizes="144x144" href="./../apple-icon-144x144.png">
    <link rel="icon" type="image/x-icon" href="./../favicon.ico">
    <link rel="icon" type="image/png" sizes="192x192" href="./../android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="96x96" href="./../favicon-96x96.png">
    <meta name="msapplication-TileColor" content="#FFD700">
    <meta name="msapplication-TileImage" content="./../ms-icon-144x144.png">
    <link rel="manifest" href="./../manifest.json">
    <meta name="keywords" content="calcolatori elettronici" />
    <meta name="description" content="Il seguente sito contiene gli appunti e le definizioni del corso 'Calcolatori elettronici T'.">
    <meta name="robots" content="index">
    <meta name="format-detection" content="telephone=no">
    <meta name="themeColor" content="#FFD700">
    <script src="./../scripts/script.js"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-78NHLXDQD8"></script>
    <script defer async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <title>Calcolatori elettronici - Progettazione DLX - Implementazione pipelined </title>
</head>
<body>
    <div class="container">
        <header class="header-wrapper">
            <div class="title-wrapper">
                <span class="title">
                    Calcolatori elettronici
                </span>
                <span class="subtitle">
                    by lorenzoarlo
                </span>
            </div>
            <span class="page-title">
                Progettazione DLX
            </span>
        </header>
        <aside class="sidebar">
            <h2 class="sidebar-title">Calcolatori elettronici</h2>
            <div class="index-container">
                <ul class="parent-ul"><li class="section-li"><a href="../index.html" rel="nofollow">Indice</a></li><li class="section-li "><a href="../funzionamento-della-cpu-in-un-elaboratore/il-modello-della-cpu.html" rel="nofollow">Funzionamento della CPU in un elaboratore</a><ul><li class="subsection-li "><a href="../funzionamento-della-cpu-in-un-elaboratore/il-modello-della-cpu.html" rel="nofollow">Il modello della CPU</a></li><li class="subsection-li "><a href="../funzionamento-della-cpu-in-un-elaboratore/comunicazione-con-dispositivi.html" rel="nofollow">Comunicazione con dispositivi</a></li><li class="subsection-li "><a href="../funzionamento-della-cpu-in-un-elaboratore/instruction-set-architecture.html" rel="nofollow">Instruction Set Architecture</a></li><li class="subsection-li "><a href="../funzionamento-della-cpu-in-un-elaboratore/il-microprocessore-dlx.html" rel="nofollow">Il microprocessore DLX</a></li><li class="subsection-li "><a href="../funzionamento-della-cpu-in-un-elaboratore/eventi-e-interruzioni.html" rel="nofollow">Eventi e interruzioni</a></li><li class="subsection-li "><a href="../funzionamento-della-cpu-in-un-elaboratore/protocollo-di-comunicazione-handshake.html" rel="nofollow">Protocollo di comunicazione handshake</a></li></ul></li><li class="section-li current">Progettazione DLX<ul><li class="subsection-li "><a href="implementazione-sequenziale.html" rel="nofollow">Implementazione sequenziale</a></li><li class="subsection-li current">Implementazione pipelined<ul><li class="definition-li"><a href="#def2-16" rel="nofollow">Pipelinining</a></li><li class="definition-li"><a href="#def2-17" rel="nofollow">Pipelinining in una CPU</a></li><li class="definition-li"><a href="#def2-18" rel="nofollow">Mutua esclusione dei componenti nell'implementazione pipeline</a></li><li class="definition-li"><a href="#def2-19" rel="nofollow">Rendere più veloce l'accesso in memoria</a></li><li class="definition-li"><a href="#def2-20" rel="nofollow">Struttura essenziale del Datapath nell'implementazione pipeline</a></li><li class="definition-li"><a href="#def2-21" rel="nofollow">Alee nell'implementazione pipeline</a></li><li class="myexample-li"><a href="#example7" rel="nofollow">Simulazione di stalli dovuti ad alee di dato</a></li><li class="definition-li"><a href="#def2-22" rel="nofollow">Gestione delle alee di controllo</a></li><li class="definition-li"><a href="#def2-23" rel="nofollow">Gestione delle alee di dato</a></li></ul></li></ul></li></ul>
            </div>
        </aside>
        <div class="sidebar-button" onclick="toggle_sidebar(this)" role="button" >
            keyboard_double_arrow_right
        </div>
        <section class="content-wrapper">
            <h1 class="section-title content-width">Implementazione pipelined</h1>
            <article class="content-container content-width">
                <div class="definition environment" id="def2-16"><h2 class="environment-title">Definizione - Pipelinining</h2><div class="environment-body">     Come visto, l'<strong>implementazione sequenziale</strong> risulta essere particolarmente lenta in quanto ogni istruzione è eseguita <strong>singolarmente</strong> e solo <strong>al termine della precedente</strong>.     <br/>     È possibile tuttavia notare che, per eseguire ogni istruzione, la CPU "deve attraversare" sempre le stesse fasi (anche se per qualche istruzione non ne esegue alcune).     Tale osservazione rende un sistema basato su CPU adatto ad essere implementato grazie al <strong>principio del pipelinining</strong>, già ampiamente utilizzato in diversi settori dell'industria.     <br/>     Per rendere più evidente le differenze (in termini di efficienza) tra un sistema sequenziale e uno in pipeline, consideriamo un sistema <span class="math-span">\( S\)</span> che deve eseguire per <span class="math-span">\( n\)</span> volte un'attività <span class="math-span">\( A\)</span> e definiamo:     <ul class="list-container"><li class="list-item">la <i>latenza</i> (<i>latency</i>) <span class="math-span">\( T_A\)</span>, ovvero il <strong>tempo che intercorre tra l'inizio ed il completamento</strong> dell'attività <span class="math-span">\( A\)</span> (in altre parole, il tempo di completamento);         </li><li class="list-item">il <i>throughput</i>, ovvero la<strong> frequenza con cui sono completate le attività</strong>;     </li></ul>     Considerando quindi un sistema sequenziale con latenza <span class="math-span">\( T_A\)</span>, si avrà un throughput pari a <span class="math-span">\( \frac{1}{T_A}\)</span> mentre, considerando un sistema in pipeline a <span class="math-span">\( x\)</span> stadi, si avrà che è possibile ottenere, considerando una latenza sempre di <span class="math-span">\( T_A\)</span>, un throughput (ideale) di <span class="math-span">\( \frac{x}{T_A}\)</span>. </div></div><div class="definition environment" id="def2-17"><h2 class="environment-title">Definizione - Pipelinining in una CPU</h2><div class="environment-body">     Adattando tale principio e considerando che     <ul class="list-container"><li class="list-item">ogni istruzione necessita di <span class="math-span">\( 5\)</span> fasi;         </li><li class="list-item">il tempo di esecuzione di ogni fase è pari a quello del clock <span class="math-span">\( T_{CLK}\)</span> (che dovrà essere quindi pari almeno al maggiore ritardo delle reti combinatorie presenti in ogni fase)     </li></ul>     si ha che la latenza è uguale a     <span class="math-block">\[         T_A = 5 \cdot T_{CLK}       \]</span>     e che il throughput sia uguale a     <span class="math-block">\[         \begin{array}{ccl}             \text{throughput} &amp; = &amp; \frac{x}{T_A} \\             &amp; = &amp; \frac{5}{5 \cdot T_{CLK}} \\             &amp; = &amp; \frac{1}{T_{CLK}}             \end{array}     \]</span>     Da ciò, si ha quindi che (idealmente) si esegue un'istruzione ad ogni clock.     <br/>     Considerando quindi la pipeline e le varie istruzioni, esse evolveranno nel seguente modo:     <div class="image-environment"><div class="image-wrapper spaced-90"><img alt="Immagine" src="../resources/pipeline-evoluzione-istruzioni.png"/></div></div>     e, ovviamente, la pipeline evolverà nel seguente modo:     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-evoluzione-pipeline.png"/></div></div> </div></div><div class="definition environment" id="def2-18"><h2 class="environment-title">Definizione - Mutua esclusione dei componenti nell'implementazione pipeline</h2><div class="environment-body">     Per implementare il pipelining correttamente, è necessario considerare alcune problematiche legate alla <strong>mutua esclusione dei componenti</strong>.     Dato che nello stesso istante ogni "fase" è potenzialmente attiva, è necessario che ogni componente sia utilizzato unicamente in uno "stadio".     <br/>     È il caso dell'ALU, il cui utilizzo per l'incremento del <span class="mono">PC</span> è in conflitto con il fatto che può essere utilizzata dall'istruzione nella fase di Execute.     Sempre riguardo al <span class="mono">PC</span>, si ha che necessario incrementare il valore di tale registro nella fase di Instruction Fetch, in quanto proprio nel ciclo di clock successivo, sarà utilizzato tale valore per conoscere l'istruzione successiva.     <br/>     Altro problema di concorrenza riguarda l'<span class="mono">MDR</span> nel caso si verifichi una Load seguita immediatamente da una Store: si avrebbe infatti che nel momento in cui il valore in memoria è stato letto ed è in attesa di essere scritto (per l'istruzione di Load), sarebbe sovrascritto dal valore in scrittura.      È quindi necessario utilizzare due <span class="mono">MDR</span>, detti appunto <span class="mono">LMDR</span> (<i><strong>L</strong>oad MDR</i>) e <span class="mono">SMDR</span> (<i><strong>S</strong>tore MDR</i>).     <br/>     Infine, per evitare di rallentare l'esecuzione, è necessario implementare l'<strong>archittetura Harvard</strong>, che prevede la <strong>separazione della memoria</strong> in <i>Instruction Memory</i> (<span class="mono">IM</span>, per l'accesso nella fase di Fetch) e in <i>Data Memory</i> (<span class="mono">DM</span>, per l'accesso in fase di Memory).      Infatti, nonostante si utilizzi una memoria unica (principale), si decide di utilizzare due memorie più piccole (e più veloci) che contengono solo un sottoinsieme dei dati.      Sarebbe possibile utilizzare la stessa memoria ma ciò porterebbe, oltre al collo di bottiglia generato dai <strong>tempi d'accesso in memoria</strong>, anche il collo di bottiglia generato d<strong>all'attesa dell'accesso in memoria</strong> (in quanto la memoria può "servire" un'ordine alla volta) che porterebbe al rallentamento intero della pipeline.     <br/>     Oltre a questi problemi di <strong>concorrenza</strong>, è necessario evidenziare la presenza dei <i>Pipeline Registers</i> che trasportano sia dati, sia informazioni di controllo (rendendo l'unità di controllo distribuita). </div></div><div class="definition environment" id="def2-19"><h2 class="environment-title">Definizione - Rendere più veloce l'accesso in memoria</h2><div class="environment-body">     Come è stato detto l'accesso in memoria può risultare problematico, in quanto genericamente prevede più di un ciclo di clock.     <h3 class="inner-title">Gerarchia delle memorie</h3>     In base alla <strong>capienza e alla velocità di accesso</strong> (tipicamente correlate), in un sistema basato su microprocessore si possono trovare le seguenti memorie:     <ul class="list-container"><li class="list-item">le memorie di massa, molto capienti ma estremamente lenta;         </li><li class="list-item">le memorie DDR, più veloci delle precedenti (ma comunque lente rispetto al processore);         </li><li class="list-item">le memorie cache <span class="mono">L3</span>, <span class="mono">L2</span>, <span class="mono">L1</span>, progressivamente meno capienti ma estremamente veloci.          Tali memorie, solitamente basate sulla tecnologia dei latch, sono situate solitamente all'interno della CPU;         </li><li class="list-item">i registri interni alla CPU (detti anche memoria <span class="mono">L0</span>), cui non è necessario effettuare alcun ciclo di bus.     </li></ul>     Dato che all'<strong>aumentare della capienza aumenta anche il tempo di accesso</strong>, si ha che le memorie cache possono contenere solo un sottoinsieme della memoria totale.      <br/>     Per rendere il processo di reperimento dei dati il più veloce possibile, si affida la gestione di ciò al <i>Memory Controller</i>, una rete che, in contemporanea, richiede il dato desiderato a tutte le memorie presenti: nel caso il dato sia presente nelle memorie più veloci, non si hanno rallentamenti sulla pipeline, altrimenti sarà necessario aspettare.     <div class="mynote environment"><h3 class="environment-title">Nota bene - Maggiore capienza, minore velocità</h3><div class="environment-body">         Oltre alla tecnologia di implementazione, un altro fattore impattante sulla velocità di accesso è la capienza della memoria.          <br/>         Si ha infatti che, all'aumentare della capienza, aumenta anche il numero di gate necessari all'indirizzamento.          Per utilizzare meno gate, un'alternativa sarebbe utilizzare<strong> più livelli in cascata</strong> (aumentando tuttavia i ritardi) mentre utilizzando l'espressione minima SP (che garantisce solo due livelli di gate) si avrebbe un <strong>maggiore spazio occupato</strong> (altro fattore da considerare quando si realizza una CPU).       </div></div>     <h3 class="inner-title">Funzionamento delle memorie cache</h3>     All'arrivo dell'indirizzo desiderato, le memorie cache verificano la presenza di tale dato (dette per questo motivo <strong>memorie associative</strong>, in quanto associano l'indirizzo ai dati) e nel caso esso sia presente (ovvero si ha un <i>hit</i>) è fornito, altrimenti (nel caso di un <i>miss</i>) si ha che è necessario richiederlo alla memoria più capiente: per farlo si effettua un <strong>trasferimento burst</strong> (che risulta essere più veloce), ovvero oltre all'indirizzo desiderato sono immagazzinati anche gli indirizzi contigui (effettuando un cosiddetto <i>line-fill</i> della cache che eliminerà le informazioni presenti in precedenza, seguendo una certa politica), dato che è molto probabile che siano quelli che verranno richiesti a breve (ad esempio nel caso dello scorrimento dei dati in un array).     <br/>     Oltre a queste informazioni, è necessario che le cache tengano conto anche della validità dei dati e che si occupino di aggiornare la memoria centrale nel caso di scritture.     <div class="mynote environment"><h3 class="environment-title">Nota bene - Vantaggi della tecnologia RISC</h3><div class="environment-body">         Un motivo per cui i processori RISC sono tipicamente più veloci rispetto ai processori CISC è la presenza di più memorie cache.          Essendo tipicamente più semplici, infatti, nei processori RISC le reti logiche per l'effettivo funzionamento del sistema occupano meno spazio permettendo quindi di inserire più memoria cache.     </div></div> </div></div><div class="definition environment" id="def2-20"><h2 class="environment-title">Definizione - Struttura essenziale del Datapath nell'implementazione pipeline</h2><div class="environment-body">     L'implementazione pipeline del DLX è sicuramente più complessa rispetto a quella sequenziale e si presenta approssimativamente nel seguente modo:     <div class="image-environment"><div class="image-wrapper spaced-95"><img alt="Immagine" src="../resources/pipeline-basic-datapath.png"/></div></div>     Come è possibile osservare, è evidente che le diverse fasi sono separate dai Pipeline Registers, che trasportano da uno stadio all'altro le informazioni necessarie.     <br/>     Per capire meglio le diverse parti, è necessario analizzarle.     <h3 class="inner-title">Fase di Instruction Fetch</h3>     La rete che compone la fase di Fetch     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-fase-if-adder.png"/></div></div>     risulta essere particolarmente semplice, in quanto non deve gestire particolari problematiche.     <br/>     È da notare che il Pipeline Pegister <span class="mono">IF / ID</span> campionerà e memorizzerà il Program Counter successivo (necessario per le istruzioni di salto) nel registro <span class="mono">PC1</span> e l'istruzione ottenuta dalla memoria nel registro <span class="mono">IR1</span>.     <br/>     Tale fase è tuttavia <strong>ulteriormente semplificabile</strong>, in quanto è possibile svolgere ogni suo compito <strong>utilizzando un contatore</strong> che rende disponibile sia lo stato presente che lo stato futuro (non tradizionale ma facilmente implementabile).      Inoltre, è possibile notare che il valore che il <span class="mono">PC</span> assume è (e deve essere) sempre multiplo di <span class="math-span">\( 4\)</span>, rendendo superflui i due bit meno significativi (in altre parole significa che è possibile utilizzare un <span class="mono">Counter x30</span>).      Date queste considerazioni, si ha che è possibile modificare lo schema nel seguente modo:     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/counter-x30-pipeline-if.png"/></div></div>     dove i segnali <span class="mono">JUMP</span> e <span class="mono">JUMP_ADDRESS[31..2]</span> provengono dallo stadio di Memory e il segnale <span class="mono"><span class="overline">STALL</span></span> proviene dall'unità di controllo.     <h3 class="inner-title">Fase di Instruction Decode</h3>     La rete che compone la fase di Instruction Decode     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-fase-id-base.png"/></div></div>     risulta essere già più complessa.      <br/>      Essa deve infatti <strong>decodificare l'istruzione</strong>, quindi <strong>prelevare i due registri sorgente</strong> ed effettuare una eventuale estensione del segno dell'immediato.      <br/>     Da notare inoltre i segnali <span class="mono">RD</span> e <span class="mono">DATA</span> in ingresso al Register File provenienti dalla fase di Write Back e utili per modificare un registro al termine di un'istruzione.     <h3 class="inner-title">Fase di Execute</h3>     La rete che compone la fase di Execute     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-fase-exe-base.png"/></div></div>     ha diversi compiti.      <br/>     Si <strong>verifica</strong>, infatti, <strong>il valore del registro che condiziona un Branch</strong> (e salvato nel registro <span class="mono">COND</span>), sono effettuate le <strong>diverse operazioni ALU</strong> (tra cui il calcolo di operazioni aritmetico-logiche, le operazioni di Set Condition, ecc.) e il <strong>calcolo dell'indirizzo di Jump Destination</strong> come somma tra il registro e l'immediato (salvate nell'apposito registro <span class="mono">X</span>, in questo caso utilizzato come <span class="mono">BTA</span> (<i><strong>B</strong>ranch <strong>T</strong>arget <strong>A</strong>ddress</i>)).      Viene infine salvato nel registro <span class="mono">SMDR</span> il <strong>valore da trasferire in una Store</strong>.      <h3 class="inner-title">Fase di Memory</h3>     La rete che compone la fase di Memory     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-fase-mem-base.png"/></div></div>     si deve occupare anche della realizzazione dei salti, i cui segnali relativi sono retroazionati alla fase di Instruction Fetch.      <br/>     In questa fase, oltre al trasporto di vari segnali, è trasportato anche il valore di <span class="mono">X</span> (depositato in <span class="mono">Y</span>) contenente, a questo punto, il risultato dell'ALU.     <h3 class="inner-title">Fase di Write Back</h3>     La rete che compone la fase di Write Back     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-fase-wb-base.png"/></div></div>     termina l'esecuzione dell'istruzione.       <br/>     Ha il compito infatti di sovrascrivere il valore calcolato al Register File, retroazionando quindi i segnali alla fase di Instruction Decode. </div></div><div class="definition environment" id="def2-21"><h2 class="environment-title">Definizione - Alee nell'implementazione pipeline</h2><div class="environment-body">     Si verifica una <strong>situazione di alea</strong> (o <i>Hazard</i>) quando, in un determinato ciclo di clock, un'<strong>istruzione presente in uno stadio della pipeline non può essere eseguita</strong> in quel determinato ciclo di clock.     In base al motivo per cui ciò si verifica, si dividono in:     <ul class="list-container"><li class="list-item"><strong>alee strutturali</strong>, quando una risorsa è condivisa tra due stati della pipeline e vi è quindi concorrenza.          Ciò avviene quando si effettuano degli accessi in memoria a degli indirizzi non presenti nelle cache: è quindi necessario effettuare in entrambi i casi un accesso alla memoria centrale (che è particolarmente lento).         Per gestire questo tipo di problematiche, si utilizza l'<strong>architettura Harvard</strong> per rendere meno frequenti gli accessi alla memoria centrale. Altro caso di alea strutturale era il caso dell'utilizzo dell'ALU per l'incremento del Program Counter (risolvibile nei modi elencati sopra);         </li><li class="list-item"><strong>alee di controllo</strong>, quando vi è un branch (ovvero non si conoscono le istruzioni che devono essere eseguite, in quanto il branch potrebbe essere taken o meno).          </li><li class="list-item"><strong>alee di dato</strong>, quando vi è una <strong>dipendenza dei dati tra le istruzioni</strong>.          Ciò avviene in una tipica situazione di RAW (<i><strong>R</strong>ead <strong>A</strong>fter <strong>W</strong>rite</i>, nominata così dal tipo di istruzioni che si devono susseguire affinché tale problema si verifichi), ovvero quando si legge un registro che dovrebbe essere modificato da un'istruzione precedente, ma che non è ancora accaduto in quanto l'istruzione non ha completato la fase di Write Back.     </li></ul>     L'approccio più banale per gestire queste alee è bloccare le istruzioni che non possono essere eseguite (ovvero vi è uno <strong>stallo della pipeline</strong>) insieme a tutte quelle che le seguono, fino a che le istruzioni entrate prima non rimuoveranno le cause dell'alea. </div></div><div class="myexample environment" id="example7"><h2 class="environment-title">Esempio - Simulazione di stalli dovuti ad alee di dato</h2><div class="environment-body collapsed">     Considerando il seguente codice, simulare l'evolversi della pipeline all'avanzare del clock     <pre class="code-block"><code>ADD R1, R4, R5
LHI R5, 0x8000
SB R4, 0x0100(R5)
SB R4, 0x0200(R1)</code></pre>     Dato questo codice, si avrà che l'evoluzione della pipeline sarà la seguente:     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-stall-diagram.png"/></div></div>     Da notare che l'unico caso di stallo si avrebbe sull'<span class="mono">istruzione 3</span>, in quanto dovrebbe utilizzare il valore del registro <span class="mono">R5</span> che sarebbe poi modificato dall'<span class="mono">istruzione 2</span>.      Non si verifica tuttavia alcuno stallo per l'<span class="mono">istruzione 4</span>, in quanto grazie allo stallo sopracitato, l'<span class="mono">istruzione 1</span> fa in tempo ad aggiornare il valore di <span class="mono">R1</span>. </div><div class="environment-tail"><span class="material-symbols-outlined body-visibility-icon" onclick="expand_environment(event)">expand_more</span></div></div><div class="definition environment" id="def2-22"><h2 class="environment-title">Definizione - Gestione delle alee di controllo</h2><div class="environment-body">     Le alee di controllo sono particolarmente frequenti e utilizzare come soluzione lo stallo (ovvero adottare una politica <i>Always Stall</i>, soluzione particolarmente facile da gestire) risulta essere particolarmente inefficiente.     Dato ciò, esistono diverse tecniche per ottimizzare questa problematica.     <h3 class="inner-title">Adottare una politica <i>Predict Not Taken</i></h3>     L'idea alla base della politica <i>Predict Not Taken</i> (alternativa della politica <i>Always Stall</i>) è molto semplice.      <br/>     Considerando un'istruzione di branch, si ha che il branch può essere sia <i>taken</i> chee <i>not taken</i>.      Dato ciò, si ha che potremmo far "evolvere naturalmente" le istruzioni in pipeline (evitando quindi lo stallo) e, solo nel caso di un salto effettivo andare ad eliminare le istruzioni che sono erroneamente entrate in pipeline.     <br/>     È da notare che l'eliminazione di un'istruzione in pipeline è facilmente attuabile <strong>andando a modificare il codice operativo di tale istruzione</strong> (e farlo corrispondere all'istruzione <span class="code-inline">NOP</span> (<i><strong>N</strong>o <strong>Op</strong>eration</i>)).     Infine, è da osservare che tale eliminazione non crea particolari problemi in quanto si ha che una certa istruzione ha un effettivo impatto nel sistema solo nelle fasi di Memory (in cui è possibile che scriva dei dati in memoria) e di Write Back (in cui può sovrascrivere il valore del Register File): tali fasi non saranno infatti mai raggiunte da queste istruzioni "errate" in quanto abbiamo posto la verifica della condizione proprio nella fase di Memory (in cui è quindi presente l'istruzione di salto).     <h3 class="inner-title">Modificare la fase in cui si verifica che il salto sia <i>taken</i></h3>     Come già detto, si ha che la verifica della condizione di salto è effettuata nella fase di Memory.      Ciò comporta quindi che le <strong>potenziali istruzioni errate entrate in pipeline</strong> (adottando una politica <i>Predict Not Taken</i>) <strong>siano tre</strong>.     <br/>     Si ha tuttavia che è possibile verificare che il salto sia <i>taken</i> anche in fasi precedenti.     <br/>     Consideriamo infatti di effettuare la verifica nella fase di Execute, in particolare colleghiamo la verifica della condizione (rappresentata dalla black-box <span class="mono">IS ZERO?</span>) direttamente alla fase di Instruction Fetch, ovvero     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-jump-2-instruction-error.png"/></div></div>     Tale soluzione ha chiaramente il vantaggio di far entrare <strong>solamente due potenziali istruzioni errate</strong>, ma può anche creare alcuni problemi legati alle temporizzazioni (in particolare riguardo alla stabilità dei segnali campionati).      Ciò potrebbe quindi rendere necessario un periodo di clock maggiore.     <br/>     Si ha inoltre la possibilità di ridurre ulteriormente il numero di istruzioni da annullare, effettuando le seguenti modifiche     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-jump-1-instruction-error.png"/></div></div>     ovvero nella fase di Fetch si posiziona un MUX (che discrimina il valore dell'istruzione da eseguire) prima di effettuare il calcolo del Program Counter successivo.      In questo modo si aggiorna (in caso di salto) immediatamente il valore di accesso nell'Instruction Memory, dovendo annullare solamente l'istruzione attualmente in fase di Instruction Decode.     Ciò permette di avere <strong>un'unica istruzione da annullare</strong>, necessitando tuttavia componenti più reattivi (o adottare una frequenza di clock minore).     <h3 class="inner-title">Delayed Branch</h3>     Un'altra modalità di gestione delle alee di controllo è l'attuazione del <i>Delayed Branch</i> da parte del <strong>compilatore</strong>.      L'idea alla base è molto semplice: spesso prima di un salto sono presenti delle istruzioni indipendenti da esso (e non contenenti altre istruzioni di salto): è quindi possibile inserirle in pipeline immediatamente dopo il salto (nei cosiddetti delay slot) al posto di potenziali istruzioni errate.     <div class="image-environment"><div class="image-wrapper spaced-60"><img alt="Immagine" src="../resources/compiled-delayed-branch.png"/></div></div>         In alcuni casi ciò potrebbe non essere possibile, dovendo sostituire alcune istruzioni con l'apposita istruzione <span class="code-inline">NOP</span>.      È ovvio che l'impiego di tale tecnica (attuata in fase di compilazione) deve essere in accordo con l'hardware che non dovrà manipolare il codice operativo delle istruzione precedenti (dato che già se ne occupa il compilatore).      <h3 class="inner-title">Dynamic Prediction con Branch Target Buffer</h3>     Un'altra tecnica per la gestione delle alee di controllo è la <i>Dynamic Prediction</i> attuata grazie al <strong>BTB</strong> (<i><strong>B</strong>ranch <strong>T</strong>arget <strong>B</strong>uffer</i>), ovvero una memoria contenente una previsione su alcune istruzioni di salto che si sono eseguite in precedenza.      Al fine di ciò, si utilizzerà una memoria associativa che fa corrispondere l'indirizzo dell'istruzione di salto al suo <i>Branch Target Address</i> e alla previsione sul fatto che tale salto sia (o meno) <i>taken</i>.     La previsione, effettuabile con diverse metodologie, porterebbe ad una situazione ottimale in cui non si ha alcuno stallo se fosse corretta, altrimenti vi sarebbe un rallentamento. </div></div><div class="definition environment" id="def2-23"><h2 class="environment-title">Definizione - Gestione delle alee di dato</h2><div class="environment-body">     Le alee di dato sono generate dalla dipendenza dei dati utilizzati da diverse istruzioni presenti in pipeline.      Tipicamente avviene in una situazione RAW (<i><strong>R</strong>ead <strong>A</strong>fter <strong>W</strong>rite</i>) ed è essenziale gestire questo tipo di alee, in quanto potrebbero generare risultati non previsti dal programmatore.      <h3 class="inner-title">Implementare il forwarding</h3>     Il <strong>forwarding</strong> è una tecnica di gestione delle alee di dato che prevede l'inserimento di una rete logica, detta <i>Forwarding Unit</i> che, avendo in ingresso il codice operativo e i registri utilizzati degli stadi successivi della pipeline, può modificare i valori letti negli stadi precedenti.     <div class="image-environment"><div class="image-wrapper spaced-90"><img alt="Immagine" src="../resources/pipeline-forwanding-unit.png"/></div></div>     Oltre alla Forwarding Unit, si introducono anche diversi multiplexer (controllati appositamente dalla Forwarding Unit) che andranno a forzare (in maniera combinatoria) un particolare segnale che è modificato da istruzioni precedenti.     <br/>     Oltre ai multiplexer presenti nella fase Execute (che riescono a risolvere gran parte delle alee), sono da notare anche i multiplexer posti sull'uscita del Register File (non collegati alla Forwarding Unit).     <br/>      Tali componenti sono infatti essenziali per gestire le alee di dato generate da un'istruzione che utilizza un valore che sarebbe modificato dall'istruzione in fase di Write Back.      A causa delle temporizzazioni, infatti, il valore campionato non sarebbe coerente generando un alea di dato.      Per risolverla si inseriscono quindi i multiplexer nella fase di Instruction Decode che (dato il fatto che hanno un comportamento combinatorio non sincrono al clock) riescono ad intercettare questo problema.     <h3 class="inner-title">Split Cycle</h3>     Considerando il problema risolto dai multiplexer nella fase di Instruction Decode, si ha che è possibile utilizzare un'altra tecnica per gestire quelle situazioni limite (in sostituzione a tali multiplexer).     Sarebbe possibile infatti utilizzare la tecnica dello <i>Split Cycle</i> che prevede di "suddividere" in<strong> due semiperiodi il clock</strong>: sarà infatti utilizzato il primo per la scrittura del registro, mentre il secondo per la lettura (ad esempio utilizzando il fronte di salita per la scrittura ed il fronte di discesa per la lettura).     <br/>     Tale metodo tuttavia necessita di una maggiore reattività dei componenti che, nel caso non fosse possibile, dovrebbe essere gestita diminuendo la frequenza di clock.     <h3 class="inner-title">Alee dipendenti da istruzioni di Load</h3>     Il forwarding riesce a risolvere la quasi totalità delle situazioni RAW, <strong>ad eccezione</strong> di quando l'<strong>istruzione che deve effettuare la modifica in memoria è una Load</strong>: si ha infatti che il valore da riportare potrebbe <strong>"arrivare" in ritardo</strong> e non rispettare le temporizzazioni corrette.      Per queste alee di dato, si decide quindi di porre in stallo le istruzioni successive.     <h3 class="inner-title">Delayed Load</h3>     Un'altra tecnica per gestire le alee dipendenti dalla Load è la <i>Delayed Load</i>, ovvero la gestione via software da parte del compilatore.      È infatti possibile inserire a seguito delle Load delle istruzioni che <strong>non dipendono in alcun modo da essa</strong>.      Nel caso ciò non fosse possibile, sono inserite delle istruzioni <span class="code-inline">NOP</span> (equivalenti ad uno stallo). </div></div>
            </article>
            <nav class="buttons-container content-width">
                <a class="navigation-button previous" href="implementazione-sequenziale.html" rel="nofollow"><span>Implementazione sequenziale</span></a>
                
            </nav>
        </section>
        <div class="scroll-to-bottom-button" onclick="scroll_to_bottom()">
            <span class="material-symbols-outlined">
                keyboard_double_arrow_down
            </span>
        </div>
            <footer class="footer-wrapper">
                <div class="copyright-wrapper">
                    <span> &copy; Copyright 2024</span> /
                    <span>made by <a href="https://github.com/lorenzoarlo" rel="nofollow">lorenzoarlo</a></span>
                </div>
                /
                <div class="privacy-wrapper">
                    <span><a href="https://lorenzoarlo.github.io/privacy-and-cookies/" rel="nofollow">Pannello preferenze
                            cookie</a></span> /
                    <span><a href="https://lorenzoarlo.github.io/privacy-and-cookies/privacy-policy.html" rel="nofollow"
                            target="_blank">Privacy Policy</a></span>
                </div>
            </footer>
    </div>
</body>
</html>