<!DOCTYPE html>
<html lang="IT">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="stylesheet" href="../styles/style.css" />
    <link rel="stylesheet" href="../styles/index-style.css" />
    <link rel="stylesheet" href="../styles/content-style.css" />
    <link rel="icon" type="image/x-icon" href="../resources/favicon.ico">
    <meta name="application-name" content="Calcolatori elettronici" />
    <meta name="apple-mobile-web-app-title" content="Calcolatori elettronici" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="#FFD700" />
    <link rel="apple-touch-icon" href="../resources/favicon.png" />
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-78NHLXDQD8"></script>
    <script src="../scripts/script.js"></script>
    <style>:root { --bg-clr: #FFD700; --fg-clr: #262626; }</style>
    <meta name="theme-color" content="#FFD700" />
    <meta name="keywords" content="calcolatori elettronici" />
    <meta name="description" content="Il seguente sito contiene gli appunti e le definizioni del corso 'Calcolatori elettronici T'.">
    <meta name="robots" content="index">
    <script defer async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>Calcolatori elettronici - Progettazione DLX - Implementazione pipelined</title>
</head>
<body>
    <div class="container">
        <header class="header-wrapper">
            <div class="title-wrapper">
                <span class="title">
                    Calcolatori elettronici
                </span>
                <span class="subtitle">
                    by lorenzoarlo
                </span>
            </div>
            <span class="page-title">
                Progettazione DLX
            </span>
        </header>
        <section class="content-wrapper">
            <h1 class="section-title content-width">Implementazione pipelined</h1>
            <article class="content-container content-width">
                <div class="definition environment" id="def2-16"><h2 class="environment-title">Definizione - Pipelinining</h2><div class="environment-body">     Come visto, l'implementazione sequenziale risulta essere particolarmente lenta in quanto ogni istruzione è eseguita singolarmente e solo al termine della precedente.     <br/>     È possibile tuttavia notare che, per eseguire ogni istruzione, la CPU "deve attraversare" sempre le stesse fasi (anche se per qualche istruzione non ne esegue alcune).     Tale osservazione rende un sistema basato su CPU adatto ad essere implementato grazie al <strong>principio del pipelinining</strong>, già ampiamente utilizzato in diversi settori dell'industria.     <br/>     Per rendere più evidente le differenze (in termini di efficienza) tra un sistema sequenziale e uno in pipeline, consideriamo un sistema <span class="math-span">\( S\)</span> che deve eseguire per <span class="math-span">\( n\)</span> volte un'attività <span class="math-span">\( A\)</span> e definiamo:     <ul class="list-container"><li class="list-item">la latenza <span class="math-span">\( T_A\)</span> (<i>latency</i>), ovvero il tempo che intercorre tra l'inizio ed il completamento dell'attività <span class="math-span">\( A\)</span> (in altre parole, il tempo di completamento);         </li><li class="list-item">il <i>throughput</i>, ovvero la frequenza con cui sono completate le attività;     </li></ul>     Considerando quindi un sistema sequenziale con latenza <span class="math-span">\( T_A\)</span>, si avrà un throughput pari a <span class="math-span">\( \frac{1}{T_A}\)</span> mentre, considerando un sistema in pipeline a <span class="math-span">\( x\)</span> stadi, si avrà che è possibile ottenere, considerando una latenza sempre di <span class="math-span">\( T_A\)</span>, un throughput (ideale) di <span class="math-span">\( \frac{x}{T_A}\)</span>. </div></div><div class="definition environment" id="def2-17"><h2 class="environment-title">Definizione - Pipelinining in una CPU</h2><div class="environment-body">     Adattando tale principio e considerando che:     <ul class="list-container"><li class="list-item">ogni istruzione necessita di <span class="math-span">\( 5\)</span> fasi;         </li><li class="list-item">il tempo di esecuzione di ogni fase è pari a quello del clock <span class="math-span">\( T_{CLK}\)</span> (che dovrà essere quindi pari almeno al maggiore ritardo delle reti combinatorie presenti in ogni fase);     </li></ul>     si ha che ciò implica che la latenza sia uguale a     <span class="math-block">\[         T_A = 5 \cdot T_{CLK}       \]</span>     e che il throughput sia uguale a     <span class="math-block">\[         \begin{array}{ccl}             \text{throughput} &amp; = &amp; \frac{x}{T_A} \\             &amp; = &amp; \frac{5}{5 \cdot T_{CLK}} \\             &amp; = &amp; \frac{1}{T_{CLK}}             \end{array}     \]</span>     Da ciò, si ha quindi che (idealmente) si esegue un'istruzione ad ogni clock.     <br/>     Considerando quindi la pipeline e le varie istruzioni, esse evolveranno nel seguente modo:     <div class="image-environment"><div class="image-wrapper spaced-90"><img alt="Immagine" src="../resources/pipeline-evoluzione-istruzioni.png"/></div></div>     e, ovviamente, la pipeline evolverà nel seguente modo:     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-evoluzione-pipeline.png"/></div></div></div></div><div class="definition environment" id="def2-18"><h2 class="environment-title">Definizione - Mutua esclusione dei componenti nell'implementazione pipeline</h2><div class="environment-body">     Per implementare il pipelining correttamente, è necessario considerare alcune problematiche legate alla mutua esclusione dei componenti.     Dato che nello stesso istante ogni "fase" è potenzialmente attiva, è necessario che ogni componente sia utilizzato unicamente in uno "stadio".     <br/>     È il caso dell'ALU, il cui utilizzo per l'incremento del <span class="mono">PC</span> è in conflitto con il fatto che può essere utilizzata dall'istruzione nella fase di Execute.     Sempre riguardo al <span class="mono">PC</span>, si ha che necessario incrementare il valore di tale registro nella fase di Instruction Fetch, in quanto proprio nel ciclo di clock successivo, sarà utilizzato tale valore per conoscere l'istruzione successiva.     <br/>     Altro problema di concorrenza riguarda l'<span class="mono">MDR</span> nel caso si verifichi una Load seguita immediatamente da una Store: si avrebbe infatti che nel momento in cui il valore in memoria è stato letto ed è in attesa di essere scritto (per l'istruzione di Load), sarebbe sovrascritto dal valore in scrittura.      È quindi necessario utilizzare due <span class="mono">MDR</span>, detti appunto <span class="mono">LMDR</span> (<i><strong>L</strong>oad MDR</i>) e <span class="mono">SMDR</span> (<i><strong>S</strong>tore MDR</i>).     <br/>     Infine, per evitare di rallentare l'esecuzione, è necessario implementare l'archittetura Harvard, che prevede la separazione della memoria in <i>Instruction Memory</i> (<span class="mono">IM</span>, per l'accesso nella fase di Fetch) e in <i>Data Memory</i> (<span class="mono">DM</span>, per l'accesso in fase di Memory).      Infatti, nonostante si utilizzi una memoria unica (principale), si decide di utilizzare due memorie più piccole (e più veloci) che contengono solo un sottoinsieme dei dati. Nel caso della memoria, infatti, sarebbe possibile utilizzare la stessa memoria ma ciò porterebbe, oltre al collo di bottiglia generato dall'accesso in memoria, al collo di bottiglia generato dall'attesa dell'accesso in memoria che porterebbe al rallentamento intero della pipeline.     <br/>     Oltre a questi problemi di concorrenza, è necessario evidenziare la presenza dei Pipeline Registers che trasportano sia dati, sia informazioni di controllo (rendendo l'unità di controllo distribuita). </div></div><div class="definition environment" id="def2-19"><h2 class="environment-title">Definizione - Rendere più veloce l'accesso in memoria</h2><div class="environment-body">     Come è stato detto, la massima frequenza di clock è legata alla "fase" più lenta e, dato ciò, si ha che l'accesso in memoria può risultare problematico.     <span class="inner-title">Gerarchia delle memorie</span>     In base alla capienza e alla velocità di accesso (tipicamente correlate), in un sistema basato su microprocessore si possono trovare le seguenti memorie:     <ul class="list-container"><li class="list-item">le memorie di massa, molto capienti ma estremamente lenta;         </li><li class="list-item">le memorie DDR, più veloci delle precedenti (ma comunque lente rispetto al processore);         </li><li class="list-item">le memorie cache <span class="mono">L3</span>, <span class="mono">L2</span>, <span class="mono">L1</span>, progressivamente meno capienti ma estremamente veloci. Tali memorie, solitamente basate sulla tecnologia dei latch, sono situate solitamente all'interno della CPU;         </li><li class="list-item">i registri interni alla CPU (detti anche memoria <span class="mono">L0</span>), cui non è necessario effettuare alcun ciclo di bus.     </li></ul>     Dato che all'aumentare della capienza aumenta anche il tempo di accesso, si ha che le memorie cache possono contenere solo un sottoinsieme della memoria totale.      Per rendere il processo di reperimento dei dati il più veloce possibile, si affida la gestione di ciò al Memory Controller, una rete che, in contemporanea, richiede il dato desiderato a tutte le memorie presenti: nel caso il dato sia presente nelle memorie più veloci, non si hanno rallentamenti sulla pipeline, altrimenti sarà necessario aspettare.     <div class="mynote environment"><h3 class="environment-title">Osservazioni personali - Maggiore capienza, minore velocità</h3><div class="environment-body">         Oltre alla tecnologia di implementazione, un altro fattore impattante sulla velocità di accesso è la capienza della memoria.          Si ha infatti che, all'aumentare della capienza, aumenta anche il numero di gate necessari all'indirizzamento.          Per utilizzare meno gate, un'alternativa sarebbe utilizzare più livelli in cascata di gate (aumentando tuttavia i ritardi) mentre utilizzando l'espressione minima SP (che garantisce solo due livelli di gate) si avrebbe un maggiore spazio occupato (altro fattore da considerare quando si realizza una CPU).       </div></div><span class="inner-title">Funzionamento delle memorie cache</span>     All'arrivo dell'indirizzo desiderato, le memorie cache verificano la presenza di tale dato (dette per questo motivo memorie associative, in quanto associano l'indirizzo ai dati) esse verificano la presenza di tale dato.     Nel caso esso sia presente (ovvero si ha un <i>hit</i>) è fornito, altrimenti (nel caso di un <i>miss</i>) si ha che è necessario richiederlo alla memoria più capiente: per farlo si effettua un trasferimento burst, ovvero oltre all'indirizzo desiderato sono immagazzinati anche gli indirizzi contigui (effettuando un cosiddetto <i>line-fill</i> della cache che eliminerà le informazioni presenti in precedenza, seguendo una certa politica), dato che è molto probabile che siano quelli che verranno richiesti a breve (ad esempio nel caso dello scorrimento dei dati in un array).     Oltre a queste informazioni, è necessario che le cache tengano conto anche della validità dei dati e che si occupino di aggiornare la memoria centrale nel caso di scritture.     <div class="mynote environment"><h3 class="environment-title">Osservazioni personali - Vantaggi della tecnologia RISC</h3><div class="environment-body">         Un motivo per cui i processori RISC sono tipicamente più veloci rispetto ai processori CISC, è legato alla presenza di più memorie cache.          Essendo tipicamente più semplici, infatti, nei processori RISC le reti logiche per l'effettivo funzionamento del sistema occupano meno spazio permettendo quindi di inserire più memoria cache.     </div></div></div></div><div class="definition environment" id="def2-20"><h2 class="environment-title">Definizione - Struttura essenziale del Datapath nell'implementazione pipeline</h2><div class="environment-body">     L'implementazione pipeline del DLX è sicuramente più complessa rispetto a quella sequenziale e si presenta approssimativamente nel seguente modo:     <div class="image-environment"><div class="image-wrapper spaced-95"><img alt="Immagine" src="../resources/pipeline-basic-datapath.png"/></div></div>     Come è possibile osservare, è evidente che le diverse fasi sono separate dai Pipeline Registers, che trasportano da uno stadio all'altro le informazioni necessarie.     Per capire meglio le diverse parti, è necessario analizzarle.     <span class="inner-title">Fase di Instruction Fetch</span>     La rete che compone la fase di Fetch     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-fase-if-adder.png"/></div></div>     risulta essere particolarmente semplice, in quanto non deve gestire particolari problematiche.     <br/>     È da notare che il pipeline register <span class="mono">IF / ID</span> campionerà e memorizzerà il Program Counter successivo (necessario per le istruzioni di salto) nel registro <span class="mono">PC1</span> e l'istruzione ottenuta dalla memoria nel registro <span class="mono">IR1</span>.     <br/>     Tale rete è tuttavia ulteriormente semplificabile, in quanto è possibile svolgere ogni suo compito utilizzando un contatore che rende disponibile sia lo stato presente che lo stato futuro.      Inoltre, è possibile notare che il valore che il <span class="mono">PC</span> assume è (e deve essere) sempre multiplo di <span class="math-span">\( 4\)</span>, rendendo superflui i due bit meno significativi (in altre parole significa che è possibile utilizzare un <span class="mono">Counter x30</span>).      Date queste considerazioni, si ha che è possibile modificare lo schema nel seguente modo:     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/counter-x30-pipeline-if.png"/></div></div>     dove i segnali <span class="mono">JUMP</span> e <span class="mono">JUMP_ADDRESS[31..2]</span> provengono dallo stadio di Memory e il segnale <span class="mono">STALL'</span> proviene dall'unità di controllo.     <span class="inner-title">Fase di Instruction Decode</span>     La rete che compone la fase di Instruction Decode     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-fase-id-base.png"/></div></div>     risulta essere già più complessa. Tale rete deve infatti decodificare l'istruzione, quindi prelevare i due registri sorgente ed effettuare una eventuale estensione del segno dell'immediato.     Da notare inoltre i segnali <span class="mono">RD</span> e <span class="mono">DATA</span> in ingresso al Register File provenienti dalla fase di Write Back e utili per modificare un registro al termine di un'istruzione.     <span class="inner-title">Fase di Execute</span>     La rete che compone la fase di Execute     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-fase-exe-base.png"/></div></div>     ha diversi compiti.      Si verifica, infatti, il valore del registro che condiziona un Branch (e salvato nel registro <span class="mono">COND</span>), sono effettuate le diverse operazioni ALU (tra cui il calcolo di operazioni aritmetico-logiche, le operazioni di Set Condition, ecc.) e il calcolo dell'indirizzo di Jump Destination come somma tra il registro e l'immediato (salvate nell'apposito registro <span class="mono">X</span>, in questo caso utilizzato come <span class="mono">BTA</span> (<i><strong>B</strong>ranch <strong>T</strong>arget <strong>A</strong>ddress</i>)).      Viene infine salvato nel registro <span class="mono">SMDR</span> il valore da trasferire in una Store.      <span class="inner-title">Fase di Memory</span>     La rete che compone la fase di Memory     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-fase-mem-base.png"/></div></div>     si deve occupare anche della realizzazione dei salti, i cui segnali relativi sono retroazionati alla fase di Instruction Fetch.      In questa fase, oltre al trasporto di vari segnali, è trasportato anche il valore di <span class="mono">X</span> (depositato in <span class="mono">Y</span>) contenente, a questo punto, il risultato dell'ALU.     <span class="inner-title">Fase di Write Back</span>     La rete che compone la fase di Write Back     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-fase-wb-base.png"/></div></div>     termina l'esecuzione dell'istruzione.      Ha il compito infatti di sovrascrivere il valore calcolato al Register File, retroazionando quindi i segnali alla fase di <span class="mono">ID</span>. </div></div><div class="definition environment" id="def2-21"><h2 class="environment-title">Definizione - Alee nell'implementazione pipeline</h2><div class="environment-body">     Si verifica una situazione di alea (o <i>Hazard</i>) quando in un ciclo di clock, un'istruzione presente in uno stadio della pipeline non può essere eseguita in quel clock.     In base al motivo per cui ciò si verifica, si dividono in:     <ul class="list-container"><li class="list-item">alee strutturali, quando una risorsa è condivisa tra due stati della pipeline e vi è quindi concorrenza.          Ciò avviene quando si effettuano degli accessi in memoria a degli indirizzi non presenti nelle cache: è quindi necessario effettuare in entrambi i casi un accesso alla memoria centrale (che è particolarmente lenta).         Per gestire questo tipo di problematiche, si utilizza l'architettura Harvard per rendere meno frequenti gli accessi alla memoria centrale;         </li><li class="list-item">alee di controllo, quando vi è un branch (ovvero non si conoscono le istruzioni che devono essere eseguite, in quanto il branch potrebbe essere taken o meno).          </li><li class="list-item">alee di dato, quando vi è una dipendenza tra le istruzioni.          Ciò avviene in una tipica situazione di RAW (<i><strong>R</strong>ead <strong>A</strong>fter <strong>W</strong>rite</i>, nominata così dal tipo di istruzioni che si devono susseguire affinché tale problema si verifichi), ovvero quando si legge un registro che dovrebbe essere modificato da un'istruzione precedente, ma che non è ancora accaduto in quanto l'istruzione non ha completato la fase di Write Back.     </li></ul>     Il modo più banale per gestire queste alee è bloccare le istruzioni che non possono essere eseguite (ovvero vi è uno stallo della pipeline) insieme a tutte quelle che le seguono, fino a che le istruzioni entrate prima non rimuoveranno le cause dell'alea. </div></div><div class="myexample environment" id="example7"><h2 class="environment-title">Esempio - Simulazione di stalli dovuti ad alee di dato</h2><div class="environment-body collapsed">     Considerando il seguente codice, simulare l'evolversi della pipeline all'avanzare del clock     <pre class="code-block"><code>ADD R1, R4, R5
LHI R5, 0x8000
SB R4, 0x0100(R5)
SB R4, 0x0200(R1)</code></pre>     Dato questo codice, si avrà che l'evoluzione della pipeline sarà la seguente:     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-stall-diagram.png"/></div></div>     Da notare che l'unico caso di stallo si avrebbe sull'<span class="mono">istruzione 3</span>, in quanto dovrebbe utilizzare il valore del registro <span class="mono">R5</span> che sarebbe poi modificato dall'<span class="mono">istruzione 2</span>.      Non si verifica tuttavia alcuno stallo per l'<span class="mono">istruzione 4</span>, in quanto grazie allo stallo sopracitato, l'<span class="mono">istruzione 1</span> fa in tempo ad aggiornare il valore di <span class="mono">R1</span>. </div><div class="environment-tail"><span class="material-symbols-outlined body-visibility-icon" onclick="expand_environment(event)">expand_more</span></div></div><div class="definition environment" id="def2-22"><h2 class="environment-title">Definizione - Gestione delle alee di controllo</h2><div class="environment-body">     Le alee di controllo sono particolarmente frequenti e utilizzare come soluzione lo stallo (ovvero adottare una politica <i>Always Stall</i>, soluzione particolarmente facile da gestire) risulta essere inefficiente.     <br/>     Dato ciò, esistono diverse tecniche per ottimizzare questa problematica.     <span class="inner-title">Adottare una politica <i>Predict Not Taken</i></span>     L'idea alla base della politica <i>Predict Not Taken</i> (alternativa della politica <i>Always Stall</i>) è molto semplice.      <br/>     Considerando un'istruzione di branch, si ha che il branch può essere sia <i>taken</i> e <i>not taken</i>.      Dato ciò, si ha che potremmo far "evolvere naturalmente" le istruzioni in pipeline (evitando quindi lo stallo) e, solo nel caso di un salto effettivo andare ad eliminare le istruzioni che sono erroneamente entrate in pipeline.     <br/>     È da notare che l'eliminazione di un'istruzione in pipeline è facilmente attuabile andando a modificare il codice operativo di tale istruzione (e farlo corrispondere all'istruzione <span class="code-inline">NOP</span> (<i><strong>N</strong>o <strong>Op</strong>eration</i>)).     Infine, è da osservare che tale eliminazione non crea particolari problemi in quanto si ha che una certa istruzione ha un effettivo impatto nel sistema solo nelle fasi di Memory (in cui è possibile che scriva dei dati in memoria) e di Write Back (in cui può sovrascrivere il valore del Register File): tali fasi non saranno infatti mai raggiunte da queste istruzioni "errate" in quanto abbiamo posto la verifica della condizione proprio nella fase di Memory (in cui è quindi presente l'istruzione di salto).     <span class="inner-title">Modificare la fase in cui si verifica che il salto sia <i>taken</i></span>     Come già detto, si ha che la verifica della condizione di salto è effettuata nella fase di Memory.      Ciò comporta quindi che le <strong>potenziali istruzioni errate entrate in pipeline</strong> (adottando una politica <i>Predict Not Taken</i>) <strong>siano tre</strong>.     <br/>     Si ha tuttavia che è possibile verificare che la condizione sia presa anche in fasi precedenti.     <br/>     Consideriamo infatti di effettuare la verifica nella fase di Execute, in particolare colleghiamo la verifica della condizione (rappresentata dalla black-box <span class="mono">IS ZERO?</span>) direttamente alla fase di Instruction Fetch, ovvero     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-jump-2-instruction-error.png"/></div></div>     Tale soluzione ha chiaramente il vantaggio di far entrare <strong>solamente due potenziali istruzioni errate</strong>, ma può anche creare alcuni problemi legati alle temporizzazioni (in particolare riguardo alla stabilità dei segnali campionati). Ciò potrebbe quindi rendere necessario un periodo di clock maggiore.     <br/>     Si ha inoltre la possibilità di ridurre ulteriormente il numero di istruzioni da annullare, effettuando le seguenti modifiche     <div class="image-environment"><div class="image-wrapper spaced-70"><img alt="Immagine" src="../resources/pipeline-jump-1-instruction-error.png"/></div></div>     ovvero si posiziona il MUX (che discrimina il valore dell'istruzione da eseguire) prima di effettuare il calcolo del Program Counter successivo. In questo modo è quindi possibile aggiornare (in caso di salto) immediatamente il valore di accesso nell'Instruction Memory, dovendo annullare solamente l'istruzione attualmente in fase di Instruction Decode.     Ciò comporta quindi di avere un'unica istruzione da annullare, comportando tuttavia la possibile necessità di un periodo di clock maggiore.             </div></div>
            </article>
            <nav class="buttons-container content-width">
                <a class="navigation-button previous" href="implementazione-sequenziale.html" rel="nofollow"><span>Implementazione sequenziale</span></a>
                
            </nav>
        </section>
        <div class="scroll-to-bottom-button" onclick="scroll_to_bottom()">
            <span class="material-symbols-outlined">
                keyboard_double_arrow_down
            </span>
        </div>
        <footer class="footer-wrapper">
            <div class="copyright-wrapper">
                <span> &copy; Copyright 2023</span> /
                <span>made by lorenzoarlo</span>
            </div>
            /
            <div class="privacy-wrapper">
                <span><a href="https://lorenzoarlo.github.io/privacy-and-cookies/" rel="nofollow">Pannello preferenze cookie</a></span> /
                <span><a href="https://lorenzoarlo.github.io/privacy-and-cookies/privacy-policy.html" rel="nofollow" target="_blank" >Privacy Policy</a></span>
            </div>
        </footer>
    </div>
</body>
</html>